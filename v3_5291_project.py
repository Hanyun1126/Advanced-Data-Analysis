# -*- coding: utf-8 -*-
"""v3_5291_project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1g9ZwPX1su7IQBip0KTczoGeIOUAIryN3
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import datetime

from sklearn.preprocessing import StandardScaler,normalize
from yellowbrick.cluster import KElbowVisualizer
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from datetime import date
from sklearn.mixture import GaussianMixture

import matplotlib.colors
pal = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']
cmap = matplotlib.colors.ListedColormap(pal)

url = "https://raw.githubusercontent.com/Hanyun1126/Advanced-Data-Analysis/main/marketing_campaign.csv"
customer_data = pd.read_csv(url, delimiter='\t').drop_duplicates()
customer_data

customer_data.info()

"""# Data Cleaning

## Check Uniqueness
"""

# Check if the 'ID' column is unique
is_column_unique = customer_data['ID'].is_unique

if is_column_unique:
    print("The 'column_name' column is unique.")
else:
    print("The 'column_name' column is not unique.")

"""## Check Missing Values"""

# Remove rows with missing values
# Only Income have missing vules, can the number of missing valus is reasonable, 
# so we can drop them, there are only 16 missing values since it is small enough we decide to drop them.
customer_data = customer_data.dropna()
print("The total number of data-points after removing the rows with missing values are:", len(customer_data))

"""## Categorical Variables Assessment"""

customer_data['Education'].unique()

customer_data['Marital_Status'].unique()

education_dict = {
    'Graduation': 'Basic education',
    'PhD': 'Higher education',
    'Master': 'Higher education',
    'Basic': 'Basic education',
    '2n Cycle': 'Basic education'
}

marital_status_dict = {
    'Single': 'Alone',
    'Together': 'Pair',
    'Married': 'Pair',
    'Divorced': 'Alone',
    'Widow': 'Alone',
    'Absurd': 'Alone',
    'Alone': 'Alone',
    'YOLO': 'Alone',
}

customer_data['Education'] = customer_data['Education'].map(education_dict)
customer_data['Marital_Status'] = customer_data['Marital_Status'].map(marital_status_dict)

customer_data

"""## Numerical variable Assessment

### Age column
- Calaculate the age of each customer
- remove outlier for age using IQR method
"""

# Age column
customer_data['Age']= 2023 - customer_data['Year_Birth']

# take a look for Age column
# Create a box plot
sns.set(rc={'figure.figsize':(15,5)})

#Let's create the list of features that we want a boxplot for
sns.boxplot(customer_data['Age'])

# Set labels for the x-axis and y-axis
plt.xlabel('Age')
plt.ylabel('Customers')

# Display the plot
plt.show()

"""IQR method: Calculate the interquartile range (IQR) of the dataset and remove any data points that fall outside of a certain range. Typically, a range of 1.5 times the IQR is used. The IQR is the difference between the 75th percentile and the 25th percentile of the data."""

# Calculate the IQR of the 'Age' column
q1 = customer_data['Age'].quantile(0.25)
q3 = customer_data['Age'].quantile(0.75)
iqr = q3 - q1

# Remove any data points that fall outside of 1.5 times the IQR
threshold = 1.5
customer_data = customer_data[(customer_data['Age'] > q1 - threshold*iqr) & (customer_data['Age'] < q3 + threshold*iqr)]

"""### Dt_Customer"""

customer_data["Dt_Customer"] = pd.to_datetime(customer_data["Dt_Customer"])
dates = []
for i in customer_data["Dt_Customer"]:
    i = i.date()
    dates.append(i)  
#Dates of the newest and oldest recorded customer
print("The newest customer's enrolment date in therecords:",max(dates))
print("The oldest customer's enrolment date in the records:",min(dates))

"""Resaonable, we do not need to do anything here.

### Income
"""

# take a look for Income column
# Create a box plot
sns.set(rc={'figure.figsize':(15,5)})

#Let's create the list of features that we want a boxplot for
sns.boxplot(customer_data['Income'])

# Set labels for the x-axis and y-axis
plt.xlabel('Income (in thousands)')
plt.ylabel('Customers')

# Display the plot
plt.show()

# Calculate the IQR of the 'column_name' column
q1 = customer_data['Income'].quantile(0.25)
q3 = customer_data['Income'].quantile(0.75)
iqr = q3 - q1

# Remove any data points that fall outside of 1.5 times the IQR
threshold = 1.5
customer_data = customer_data[(customer_data['Income'] > q1 - threshold*iqr) & (customer_data['Income'] < q3 + threshold*iqr)]

"""### children column"""

# Children column
# because they have the same meaning
customer_data['Children'] = customer_data['Kidhome'] + customer_data['Teenhome']

"""### Recency"""

# take a look for Income column
# Create a box plot
sns.set(rc={'figure.figsize':(15,5)})

#Let's create the list of features that we want a boxplot for
sns.boxplot(customer_data['Recency'])

# Set labels for the x-axis and y-axis
plt.xlabel('Recency')
plt.ylabel('Customers')

# Display the plot
plt.show()

# Calculate the IQR of the 'column_name' column
q1 = customer_data['Recency'].quantile(0.25)
q3 = customer_data['Recency'].quantile(0.75)
iqr = q3 - q1

# Remove any data points that fall outside of 1.5 times the IQR
threshold = 1.5
customer_data = customer_data[(customer_data['Recency'] > q1 - threshold*iqr) & (customer_data['Recency'] < q3 + threshold*iqr)]

"""## Feature Engineering
Let's create the following features:

- Age = Current year-Year_Birth
- Total_Sales = MntWines+ MntFruits + MntMeatProducts + - - MntFishProducts + MntSweetProducts + MntGoldProds
- Total_Purchases = NumDealsPurchases + NumWebPurchases + - NumCatalogPurchases + NumStorePurchases
- Avg_Purchase = Total_Sales/Total_Purchases
- Deal_Share = NumDealsPurchases/Total_Purchases
- Web_Share = NumWebPurchases/Total_Purchases
- Catalog_Share = NumCatalogPurchases/Total_Purchases
- Store_Share = NumStorePurchases/Total_Purchases
- Family_Size = number of marriage + number of children
"""

#Total_Sales = MntWines+ MntWines + MntMeatProducts + MntFishProducts + MntSweetProducts + MntGoldProds
customer_data['Total_Sales']=customer_data['MntWines']+customer_data['MntFruits']+ \
                            customer_data['MntMeatProducts']+customer_data['MntFishProducts']+ \
                            customer_data['MntSweetProducts']+customer_data['MntGoldProds']

#Total_Purchases = NumDealsPurchases + NumWebPurchases + NumCatalogPurchases + NumStorePurchases
customer_data['Total_Purchases']=customer_data['NumDealsPurchases']+customer_data['NumWebPurchases']+ \
                            customer_data['NumCatalogPurchases']+customer_data['NumStorePurchases']

#Avg_Purchase = Total_Sales/Total_Purchases
customer_data['Avg_Purchase']=customer_data['Total_Sales']/customer_data['Total_Purchases']

#Deal_Share = NumDealsPurchases/Total_Purchases
customer_data['Deal_Share']=customer_data['NumDealsPurchases']/customer_data['Total_Purchases']

#Web_Share = NumWebPurchases/Total_Purchases
customer_data['Web_Share']=customer_data['NumWebPurchases']/customer_data['Total_Purchases']

#Catalog_Share = NumCatalogPurchases/Total_Purchases
customer_data['Catalog_Share']=customer_data['NumCatalogPurchases']/customer_data['Total_Purchases']

#Store_Share = NumStorePurchases/Total_Purchases
customer_data['Store_Share']=customer_data['NumStorePurchases']/customer_data['Total_Purchases']

##Feature for total members in the householde
customer_data["Family_Size"] = customer_data["Marital_Status"].replace({"Alone": 1, "Pair":2})+ customer_data["Children"]

customer_data['Seniority']=pd.to_datetime(customer_data['Dt_Customer'], dayfirst=True,format = '%Y-%m-%d')
customer_data['Seniority'].sort_values()

last_date = date(2014,12,6)
customer_data['Seniority'] = pd.to_numeric(customer_data['Seniority'].dt.date.apply(lambda x: (last_date - x)).dt.days, downcast='integer')/30

customer_data = customer_data[customer_data['Total_Purchases']>0]
#let's look at the data to confirm that there are no longer purchases of 0 and avg purchase = inf
customer_data

"""### transformation for numerical variable"""

drop_features = ['Education','Marital_Status','Dt_Customer','ID','Year_Birth']
#Get data for scaling by removing boolean columns
df_before_scale = customer_data.drop(drop_features, axis = 1) #new data frame with numeric

# create an instance of the StandardScaler class
scaler = StandardScaler()

# fit the scaler to the data and transform it
scaled_data = scaler.fit_transform(df_before_scale)

# create a new dataframe with the scaled data
df_scaled = pd.DataFrame(scaled_data, columns=df_before_scale.columns)

df_scaled.head()

# extract new features
new_features = ['Income','Recency','Total_Sales','NumWebVisitsMonth','Avg_Purchase','Deal_Share','Web_Share','Catalog_Share','Store_Share','Family_Size']
df_scaled = df_scaled.loc[:,new_features]
df_scaled

"""### transformation for categorical variable"""

cat_features = ['Education','Marital_Status']

# apply one-hot encoding to the categorical features
df_2 = pd.get_dummies(customer_data, columns=cat_features)
df_2 = df_2.loc[:, 
                ['Education_Basic education',
                 'Education_Higher education',
                 'Marital_Status_Alone',
                 'Marital_Status_Pair']]

df_2 = df_2.reset_index()
df_2

# Join the existing df_scaled dataframe with df_age_binned.
# Store the result back into df_scaled
merged_df = pd.concat([df_scaled, df_2],axis = 1).drop(['index'], axis=1)

merged_df

"""# PCA

"""

# Fit our standardized data using PCA
pca = PCA()

# Fit PCA on scaled data
pca.fit(merged_df)

#Plot variance ratio of PCs
plt.bar(range(1, pca.n_components_ + 1), pca.explained_variance_ratio_)
plt.xlabel('Principal Component')
plt.ylabel('Variance Ratio')
plt.show()

print('Percentage of cumulative variance of PCs: ', pca.explained_variance_ratio_.cumsum() * 100)

#Plot cummulative explained variance
plt.plot(range(1, pca.n_components_ + 1), pca.explained_variance_ratio_.cumsum(), marker = 'o', linestyle = '--')
plt.xlabel('Number of components')
plt.ylabel('Cummulative explained variance')
plt.show()

"""Note:
The number of components we choose ultimately depends on our preference. Typically, it is recommended that the total variance explained by all components should be between 70% to 80%, which would equate to roughly 6 components in this case. Despite this, for the present example, only 3 components will be selected, which should preserve a little over 55% of the variance. While this may not be ideal in social sciences, it is still a reasonable level of variance retention. PC1 is responsible for around 36% of the overall variation in the dataset and has the greatest impact on it.

## Perform PCA with the chosen number of components
"""

# We choose 3 components
pca = PCA(n_components = 3)

# Fit the model with our data with the 3 selected components
pca.fit(merged_df)

# The calculated resulting components scores for the elements in our data set:
df_pca = pca.transform(merged_df)
df_pca = pd.DataFrame(df_pca, columns=['D1', 'D2', 'D3'])
df_pca

loadings = pca.components_

#create a dataframe of PC loadings/scores
df_loadings = pd.DataFrame(loadings.T)

# 16 PCs scores with 23 col names
#df_loadings.insert(0, 'col_name', df_before_scale.columns)
df_loadings.insert(0, 'col_name', merged_df.columns)
df_loadings

"""The variables Income and Total_Sales make the most significant contribution to PC1 (Avg_Purchase is also based on Total_Sales). The variables Store_Share and Web_Share are the primary contributors to PC2."""

#Plot PCA
#A 3D Projection Of Data In The Reduced Dimension
x =df_pca["D1"]
y =df_pca["D2"]
z =df_pca["D3"]
#To plot
fig = plt.figure(figsize=(10,8))
a = fig.add_subplot(111, projection="3d")
a.scatter(x,y,z, c="maroon", marker="o" )
a.set_title("A 3D Projection Of Data In The Reduced Dimension", size=15)
plt.show()

"""# Kmeans

Brief Clustering
- Stars: Old customers with high income and high spending nature.
- Neet Attention: New customers with below-average income and low - spending nature.
- High Potential: New customers with high income and high spending nature.
- Leaky Bucket: Old customers with below-average income and a low spending nature.
"""

scaler=StandardScaler()
dataset_temp = customer_data[['Income','Seniority','Total_Sales']]
X_std=scaler.fit_transform(dataset_temp)
X = normalize(X_std,norm='l2')
gmm=GaussianMixture(n_components=5, covariance_type='spherical',max_iter=2000, random_state=5).fit(X)
labels = gmm.predict(X)
dataset_temp['Cluster'] = labels
dataset_temp=dataset_temp.replace({0:'Cluster1',1:'Cluster2',2:'Cluster3',3:'Cluster4',4:'Cluster5'})
customer_data['Cluster']=dataset_temp['Cluster']

pd.options.display.float_format = "{:.0f}".format
summary=customer_data[['Income','Total_Sales','Seniority','Cluster']]
summary.set_index("Cluster", inplace = True)
summary=summary.groupby('Cluster').describe().transpose()
summary

Elbow_M = KElbowVisualizer(KMeans(), k=10)
Elbow_M.fit(df_pca)
Elbow_M.show()

# Specify the number of clusters to create
n_clusters = 5

# Create the KMeans model
kmeans = KMeans(n_clusters=n_clusters)

# Fit the model to the data
kmeans.fit(merged_df)

# Get the cluster labels
labels = kmeans.labels_

# Add the cluster labels to the original dataframe
customer_data['Cluster_5'] = labels

customer_data.head(5)

#Plotting countplot of clusters
df1=customer_data.sort_values(by='Cluster')
pl = sns.countplot(x=df1['Cluster'], palette= pal)
pl.set_title("Distribution Of The Clusters", pad=10, size = 15)
plt.show()

#Define name of customer groups according to clusters
re_clust = {
    'Cluster1':'Ordinary',
    'Cluster2':'Potential',
    'Cluster3':'Good',
    'Cluster4':'Elite',
    'Cluster5':'Stars'}
df1['Clusters_Customers'] = df1['Cluster'].map(re_clust)
df1

#Plot clusters of customers based on Income and Spent
plt.figure(figsize=(8, 6))
pl = sns.scatterplot(data = df1, x = df1['Total_Sales'], y = df1['Income'], hue = df1['Clusters_Customers'], palette = pal, s=50)
pl.set_title('Customers clusters based on Income and Spent', pad=10, size=15)
plt.legend(loc = 'upper right', bbox_to_anchor=(1.4, 1))
plt.show()

"""Clustering
- Cluster 1 (Ordinary): low spending and low income
- Cluster 2 (Potential): low spending and below average income
- Cluster 3 (Good): low spending and average income
- Cluster 4 (Elite): high spending and above average income
- Cluster 5 (Stars): high spending and high income

Clustering based on shoping
- Deal_Share
- Wed_Share
- Catalog_Share
- Store_Share
"""

spending_by_product = df1.groupby('Clusters_Customers')[['Deal_Share','Web_Share','Catalog_Share','Store_Share']].sum()

#Transpose the result
spending_by_product = spending_by_product.transpose()

#Reset the index 
spending_by_product = spending_by_product.reset_index()

#Rename column
spending_by_product = spending_by_product.rename(columns = {'index': 'Category'})

spending_by_product

# Plot bar charts
#Creates a figure with 4 subplots using the subplots() function from matplotlib
fig = plt.figure(figsize=(14, 12)) 

#Initializes k=1 and creates a list cl of strings representing the names of each customer cluster
k = 1
cl = ['Ordinary','Potential','Good','Elite','Stars']

#
for i in cl:
    #only the columns Category and i are selected, where i is the current iteration of the cl list 
    ass = spending_by_product[['Category', i]]
    
    #create a subplot in a grid with 2 rows and 2 columns, and it sets the current subplot to the k position
    plt.subplot(3, 2, k)
    plt.title(i, size = 20, x = 0.5, y = 1.03)
    plt.grid(color = 'gray', linestyle = '-', axis = 'y', alpha = 0.1)
    cluster_color = pal[k-1]
    
    #create barplot using seaborn where x is products in the category, y is the current iteration of the cl list
    a = sns.barplot(data = ass, x = 'Category', y = i, color = cluster_color, linestyle = "-", linewidth = 1, edgecolor = "black")
    plt.xticks(size = 13, color = 'black')
    plt.yticks(size = 13, color = 'black')
    plt.xlabel('')
    plt.ylabel('')
    
    #add annotations to the barplot by percentage of spending amount
    for p in a.patches:
        #set the x-coordinate to the center of the current bar patch
        x_pos = p.get_x() + p.get_width() / 2
        
        #get the height of the current patch
        y_pos = p.get_height()
        
        #calculate percentage of spending amount (current patch) and the total spending of the group
        percentage = round((y_pos / sum(ass[i])) * 100, 1)
        a.annotate(
            f'{percentage}%', #text to be displayed
            xy=(x_pos, y_pos), #position to be placed
            ha='center', va='center', #alignment of the text
            size=13, 
            color='black',
            xytext = (0, 10), 
            textcoords = 'offset points')
    
    #hide spines of the barplot (the right, top, left and bottom borders)
    for spine in ['right', 'top', 'left', 'bottom']:
        a.spines[spine].set_visible(False)
        
    #increment k after iteration of the loop
    k += 1

plt.figtext(0.2, 1.05, 'Spending of different customer groups', size = 25)
fig.tight_layout(h_pad = 3)
plt.show()





